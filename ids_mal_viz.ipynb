{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization of non-benign connections data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the top modules that are used in multiple places\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column name mapping from original data to compact form\n",
    "# All the X** are features and the YY is the label\n",
    "feature_map = {\n",
    " ' Destination Port' : 'X1',\n",
    " ' Flow Duration' : 'X2', \n",
    " ' Total Fwd Packets' : 'X3', \n",
    " ' Total Backward Packets' : 'X4', \n",
    " 'Total Length of Fwd Packets' : 'X5', \n",
    " ' Total Length of Bwd Packets' : 'X6', \n",
    " ' Fwd Packet Length Max' : 'X7', \n",
    " ' Fwd Packet Length Min' : 'X8', \n",
    " ' Fwd Packet Length Mean' : 'X9', \n",
    " ' Fwd Packet Length Std' : 'X10', \n",
    " 'Bwd Packet Length Max' : 'X11', \n",
    " ' Bwd Packet Length Min' : 'X12', \n",
    " ' Bwd Packet Length Mean' : 'X13', \n",
    " ' Bwd Packet Length Std' : 'X14', \n",
    " 'Flow Bytes/s' : 'X15', \n",
    " ' Flow Packets/s' : 'X16', \n",
    " ' Flow IAT Mean' : 'X17', \n",
    " ' Flow IAT Std' : 'X18', \n",
    " ' Flow IAT Max' : 'X19', \n",
    " ' Flow IAT Min' : 'X20', \n",
    " 'Fwd IAT Total' : 'X21', \n",
    " ' Fwd IAT Mean' : 'X22', \n",
    " ' Fwd IAT Std' : 'X23', \n",
    " ' Fwd IAT Max' : 'X24', \n",
    " ' Fwd IAT Min' : 'X25', \n",
    " 'Bwd IAT Total' : 'X26', \n",
    " ' Bwd IAT Mean' : 'X27', \n",
    " ' Bwd IAT Std' : 'X28', \n",
    " ' Bwd IAT Max' : 'X29', \n",
    " ' Bwd IAT Min' : 'X30', \n",
    " 'Fwd PSH Flags' : 'X31', \n",
    " ' Bwd PSH Flags' : 'X32', \n",
    " ' Fwd URG Flags' : 'X33', \n",
    " ' Bwd URG Flags' : 'X34', \n",
    " ' Fwd Header Length' : 'X35', \n",
    " ' Bwd Header Length' : 'X36', \n",
    " 'Fwd Packets/s' : 'X37', \n",
    " ' Bwd Packets/s' : 'X38', \n",
    " ' Min Packet Length' : 'X39', \n",
    " ' Max Packet Length' : 'X40', \n",
    " ' Packet Length Mean' : 'X41', \n",
    " ' Packet Length Std' : 'X42', \n",
    " ' Packet Length Variance' : 'X43', \n",
    " 'FIN Flag Count' : 'X44', \n",
    " ' SYN Flag Count' : 'X45', \n",
    " ' RST Flag Count' : 'X46', \n",
    " ' PSH Flag Count' : 'X47', \n",
    " ' ACK Flag Count' : 'X48', \n",
    " ' URG Flag Count' : 'X49', \n",
    " ' CWE Flag Count' : 'X50', \n",
    " ' ECE Flag Count' : 'X51', \n",
    " ' Down/Up Ratio' : 'X52', \n",
    " ' Average Packet Size' : 'X53', \n",
    " ' Avg Fwd Segment Size' : 'X54', \n",
    " ' Avg Bwd Segment Size' : 'X55', \n",
    " ' Fwd Header Length.1' : 'X56', \n",
    " 'Fwd Avg Bytes/Bulk' : 'X57', \n",
    " ' Fwd Avg Packets/Bulk' : 'X58', \n",
    " ' Fwd Avg Bulk Rate' : 'X59', \n",
    " ' Bwd Avg Bytes/Bulk' : 'X60', \n",
    " ' Bwd Avg Packets/Bulk' : 'X61', \n",
    " 'Bwd Avg Bulk Rate' : 'X62', \n",
    " 'Subflow Fwd Packets' : 'X63', \n",
    " ' Subflow Fwd Bytes' : 'X64', \n",
    " ' Subflow Bwd Packets' : 'X65', \n",
    " ' Subflow Bwd Bytes' : 'X66', \n",
    " 'Init_Win_bytes_forward' : 'X67', \n",
    " ' Init_Win_bytes_backward' : 'X68', \n",
    " ' act_data_pkt_fwd' : 'X69', \n",
    " ' min_seg_size_forward' : 'X70', \n",
    " 'Active Mean' : 'X71', \n",
    " ' Active Std' : 'X72', \n",
    " ' Active Max' : 'X73', \n",
    " ' Active Min' : 'X74', \n",
    " 'Idle Mean' : 'X75', \n",
    " ' Idle Std' : 'X76', \n",
    " ' Idle Max' : 'X77', \n",
    " ' Idle Min' : 'X78', \n",
    " ' Label': 'YY'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label names (YY) in the data and their\n",
    "# mapping to numerical values\n",
    "label_map = {\n",
    " 'BENIGN' : 0,\n",
    " 'FTP-Patator' : 1,\n",
    " 'SSH-Patator' : 2,\n",
    " 'DoS slowloris' : 3,\n",
    " 'DoS Slowhttptest': 4,\n",
    " 'DoS Hulk' : 5,\n",
    " 'DoS GoldenEye' : 6,\n",
    " 'Heartbleed' : 7,\n",
    " 'Web Attack � Brute Force' : 8,\n",
    " 'Web Attack � XSS' : 9,\n",
    " 'Web Attack � Sql Injection' : 10,\n",
    " 'Infiltration' : 11,\n",
    " 'Bot' : 12,\n",
    " 'PortScan' : 13,\n",
    " 'DDoS' : 14,\n",
    "}\n",
    "\n",
    "num_ids_features = 78\n",
    "num_ids_classes = 15\n",
    "ids_classes = [ 'BENIGN', 'FTP-Patator', 'SSH-Patator', 'DoS slowloris', 'DoS Slowhttptest', 'DoS Hulk', 'DoS GoldenEye', 'Heartbleed', 'Brute Force', 'XSS', 'Sql Injection', 'Infiltration', 'Bot', 'PortScan', 'DDoS',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = './MachineLearningCVE/restart/'\n",
    "mal_data = 'mal.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(feature_map.values())\n",
    "label = feature_names.pop()\n",
    "\n",
    "mal_class_txt = ids_classes[1:]\n",
    "num_mal_classes = num_ids_classes - 1\n",
    "mal_class_num = np.arange(1, num_mal_classes + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(outdir + mal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some rows with 'YY' = 5 have the 'X15' column as NaN.\n",
    "Set these to the average value of the 'X15' column for 'YY' = 5\n",
    "\"\"\"\n",
    "# Columns containing nan\n",
    "df.columns[df.isna().any()].tolist()\n",
    "\n",
    "# Only 'YY' = 5 has NaNs\n",
    "(df[df['X15'].isna()])['YY'].unique()\n",
    "\n",
    "avg_x15 = df[df['YY'] == 5]['X15'].mean()\n",
    "df.fillna(avg_x15, inplace=True)\n",
    "\n",
    "if len(df[df['X15'].isna()]) == 0:\n",
    "    print ('NaNs replaced with mean')\n",
    "else:\n",
    "    print ('something went wrong with NaN handling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some rows with 'YY' = 12, 13, 14, 1, 5 have inf in the X15 and X16 columns\n",
    "Replace these with the column max\n",
    "\"\"\"\n",
    "# Columns containing inf\n",
    "df.columns.to_series()[np.isinf(df).any()]\n",
    "\n",
    "# labels corresponding to rows with inf\n",
    "df.iloc[df.index[np.isinf(df).any(1)]]['YY'].unique()\n",
    "\n",
    "# replace infs in x15 column with max non-inf value\n",
    "max_x15 = df.loc[df['X15'] != np.inf, 'X15'].max()\n",
    "print (max_x15)\n",
    "df['X15'].replace(np.inf, max_x15, inplace=True)\n",
    "\n",
    "# replace infs in x16 column with max non-inf value\n",
    "max_x16 = df.loc[df['X16'] != np.inf, 'X16'].max()\n",
    "print (max_x16)\n",
    "df['X16'].replace(np.inf, max_x16, inplace=True)\n",
    "\n",
    "if len(df.columns.to_series()[np.isinf(df).any()]) == 0:\n",
    "    print ('infs replaced with max')\n",
    "else:\n",
    "    print ('something went wrong with inf handling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset is badly skewed.\n",
    "\"\"\"\n",
    "df.groupby(['YY'])['YY'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the medians of the 14 classes across all dimensions.\n",
    "\"\"\"\n",
    "\n",
    "df_median = df.groupby(['YY'])[feature_names].median()\n",
    "\n",
    "print (df_median.shape)\n",
    "print (df_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PCA\n",
    "Plot them in 2D.\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "mod_pca = PCA(n_components=10)\n",
    "X_pca = mod_pca.fit_transform(df_median)\n",
    "print (X_pca.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_pca[:, 4], X_pca[:, 5])\n",
    "\n",
    "for i, txt in enumerate(mal_class_num):\n",
    "    ax.annotate(txt, (X_pca[i, 4], X_pca[i, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi Dimensional Scaling (MDS)\n",
    "Plot them in 2D.\n",
    "\"\"\"\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "mod_mds = MDS(n_components=3, n_init=10, verbose=1, random_state=42)\n",
    "X_mds = mod_mds.fit_transform(df_median)\n",
    "print (X_mds.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_mds[:, 0], X_mds[:, 1])\n",
    "\n",
    "for i, txt in enumerate(mal_class_num):\n",
    "    ax.annotate(txt, (X_mds[i, 0], X_mds[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression to understand the most important parameters\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mod_scaler = StandardScaler()\n",
    "X_std = mod_scaler.fit_transform(df[feature_names])\n",
    "mod_log = LogisticRegression(max_iter=10)\n",
    "mod_log.fit(X_std, df['YY'])\n",
    "\n",
    "y_pred = mod_log.predict(X_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX - need to updated to python 3.10 and latest versions of scikit learn for better API\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(df['YY'], y_pred, labels=mod_log.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mod_log.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-harbor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
