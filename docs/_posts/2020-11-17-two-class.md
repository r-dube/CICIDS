---
title: Retrying principal component analysis and gaussian mixture models
---
We create a simpler dataset and use principal component analysis (PCA) and gaussian mixture models (GMMs) over this dataset.

### The simpler dataset
First, we modify our data processing script ([^scripts1]) to create a small dataset, ([^data3]) with just two classes. The first class is the BENIGN class (label 0), and the second is the PortScan class (label 11). Both classes have 8,000 samples for a total of 16,000 rows in the dataset.

The main idea here is to reduce the complexity of visualizing multiple classes in the same chart. We hope that the simplification will improve our understanding of the features in the data.
 
### Dimensionality reduction with PCA
Next, we reduce the dimensionality of the data using PCA ([^colab6]) and plot the actual labels (as colors) against the principal components (two components at a time). From the plots, we see that the BENIGN class (regular traffic on the network) has a lot more variability than the PortScan class (where an attacker uses a software tool to probe servers for open ports). Intuitively, the variability observation makes sense as the natural traffic on a network encompasses many different applications and user types. Attackers typically seek to hide their traffic within this natural variability.

Principal components one, two
![Principal components one, two](/CICIDS/assets/images/2020-11-17-twoclass-1.png)

Principal components three, four
![Principal components three, four](/CICIDS/assets/images/2020-11-17-twoclass-2.png)

Principal components five, six
![Principal components five, six](/CICIDS/assets/images/2020-11-17-twoclass-3.png)

### Sanity check with logistic regression
We do a quick sanity check with logistic regression. Logistic regression can separate out the two classes with 99%+ accuracy (on a test set, separate from the training set; there is no validation set in this experiment).

### GMMs do not fit
Finally, we fit a GMM against an unlabeled version of the dataset. We find that GMM badly under-fits, assigning 2,452 samples to one class and 13,548 to the other. A great fit would find ~8,000 samples for both classes. At least in the case of the two-class dataset, we conclude that GMM is not a reasonable method to discover the structure of the CICIDS2017 data.

### References
[^scripts1]: [Data processing script for a local machine](https://github.com/r-dube/CICIDS/blob/main/scripts/ids_utils.py)
[^data3]: [Two-class attack data](https://github.com/r-dube/CICIDS/blob/main/MachineLearningCVE/processed/twoclass-cicids2017.csv)
[^colab6]: [Experimentation wth two classes on Colab](https://github.com/r-dube/CICIDS/blob/main/ids_twoclass.ipynb)
