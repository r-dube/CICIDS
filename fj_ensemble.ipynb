{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fj_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlyc5RFd8wAVV8dI0KUlco",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-dube/CICIDS/blob/main/fj_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFWTUL3GCHsG"
      },
      "source": [
        "# Load the modules used\n",
        "import numpy as np\n",
        "import scipy as sci\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Input\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant \n",
        "from keras.optimizers import Adam\n",
        "from keras import metrics\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK7zNIN2x-DP",
        "outputId": "4959f566-bde7-46a2-d59f-c3a9fad15d94"
      },
      "source": [
        "# NLTK to remove stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nch9McXo45dY",
        "outputId": "837d18df-49ef-4449-a54f-a1dfa5baac21"
      },
      "source": [
        "# list devices\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 6379335709987962474\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 6935497038647223039\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55AqcsYyb-aa"
      },
      "source": [
        "# For reproducible results\n",
        "# except for variability introduced by GPU\n",
        "import random as rn\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '42'\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # avoid using GPU for reproducible results\n",
        "np.random.seed(42)\n",
        "rn.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFdTIEZ6Z8ZR"
      },
      "source": [
        "# For transformers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrnTamUHDzxI"
      },
      "source": [
        "# Set data_url, the location of the data\n",
        "# Data is not loaded from a local file\n",
        "# data_url=\"https://raw.githubusercontent.com/r-dube/fakejobs/main/data/fj_small.csv\"\n",
        "# data_url=\"https://raw.githubusercontent.com/r-dube/fakejobs/main/data/fj_medium.csv\"\n",
        "data_url=\"https://raw.githubusercontent.com/r-dube/fakejobs/main/data/fake_job_postings.csv\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usYPD_l1Bimz"
      },
      "source": [
        "def fj_load_df_from_url():\n",
        "    \"\"\"\n",
        "    Load dataframe from csv file\n",
        "    Input:\n",
        "        None\n",
        "    Returns:\n",
        "        dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(data_url)\n",
        "\n",
        "    print ('Loaded dataframe shape', df.shape)\n",
        "\n",
        "    counts = fj_label_stats(df)\n",
        "    print ('Not fraudulent', counts[0], 'Fraudulent', counts[1])\n",
        "\n",
        "    print(df.describe())\n",
        "\n",
        "    print ('NAs/NANs in data =>')\n",
        "    print(df.isna().sum())\n",
        "\n",
        "    return df\n",
        "\n",
        "def fj_label_stats(df):\n",
        "    \"\"\"\n",
        "    Very basic label statistics\n",
        "    Input: \n",
        "        Dataframe\n",
        "    Returns:\n",
        "        Number of samples with 0, 1 as the label\n",
        "    \"\"\"\n",
        "    counts = np.bincount(df['fraudulent'])\n",
        "    return counts\n",
        "\n",
        "def fj_txt_only(df):\n",
        "    \"\"\"\n",
        "    Combine all the text fields, discard everything else except for the label\n",
        "    Input: \n",
        "        Dataframe\n",
        "    Returns:\n",
        "        Processed dataframe\n",
        "    \"\"\"\n",
        "    \n",
        "    df.fillna(\" \", inplace = True)\n",
        "\n",
        "    df['text'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + \\\n",
        "    ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + \\\n",
        "    df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + \\\n",
        "    ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function'] \n",
        "\n",
        "    del df['title']\n",
        "    del df['location']\n",
        "    del df['department']\n",
        "    del df['company_profile']\n",
        "    del df['description']\n",
        "    del df['requirements']\n",
        "    del df['benefits']\n",
        "    del df['employment_type']\n",
        "    del df['required_experience']\n",
        "    del df['required_education']\n",
        "    del df['industry']\n",
        "    del df['function']  \n",
        "    \n",
        "    del df['salary_range']\n",
        "    del df['job_id']\n",
        "    del df['telecommuting']\n",
        "    del df['has_company_logo']\n",
        "    del df['has_questions']\n",
        "\n",
        "    return df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lks9Mm0Tc1l2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef3e5d8-9a71-4303-a221-a3ab242aabd3"
      },
      "source": [
        "df = fj_load_df_from_url()\n",
        "df = fj_txt_only(df)\n",
        "print('Maximum text length', df['text'].str.len().max())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded dataframe shape (17880, 18)\n",
            "Not fraudulent 17014 Fraudulent 866\n",
            "             job_id  telecommuting  ...  has_questions    fraudulent\n",
            "count  17880.000000   17880.000000  ...   17880.000000  17880.000000\n",
            "mean    8940.500000       0.042897  ...       0.491723      0.048434\n",
            "std     5161.655742       0.202631  ...       0.499945      0.214688\n",
            "min        1.000000       0.000000  ...       0.000000      0.000000\n",
            "25%     4470.750000       0.000000  ...       0.000000      0.000000\n",
            "50%     8940.500000       0.000000  ...       0.000000      0.000000\n",
            "75%    13410.250000       0.000000  ...       1.000000      0.000000\n",
            "max    17880.000000       1.000000  ...       1.000000      1.000000\n",
            "\n",
            "[8 rows x 5 columns]\n",
            "NAs/NANs in data =>\n",
            "job_id                     0\n",
            "title                      0\n",
            "location                 346\n",
            "department             11547\n",
            "salary_range           15012\n",
            "company_profile         3308\n",
            "description                1\n",
            "requirements            2695\n",
            "benefits                7210\n",
            "telecommuting              0\n",
            "has_company_logo           0\n",
            "has_questions              0\n",
            "employment_type         3471\n",
            "required_experience     7050\n",
            "required_education      8105\n",
            "industry                4903\n",
            "function                6455\n",
            "fraudulent                 0\n",
            "dtype: int64\n",
            "Maximum text length 14991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxtcnlHBpPro"
      },
      "source": [
        "# Utilities to clean text\n",
        "\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "    return url.sub(r\"\", text)\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r\"<.*?>\")\n",
        "    return html.sub(r\"\", text)\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE,\n",
        "    )\n",
        "    return emoji_pattern.sub(r\"\", string)\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return text.translate(table)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgdEvka4wERJ"
      },
      "source": [
        "stop = set(stopwords.words(\"english\"))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "\n",
        "    return \" \".join(text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrqwWWQ2uruM"
      },
      "source": [
        "# clean text\n",
        "df['text'] = df['text'].map(lambda x: remove_URL(x))\n",
        "df['text'] = df['text'].map(lambda x: remove_html(x))\n",
        "df['text'] = df['text'].map(lambda x: remove_emoji(x))\n",
        "df['text'] = df['text'].map(lambda x: remove_punct(x))\n",
        "df['text'] = df[\"text\"].map(remove_stopwords)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUCnYLkBpnNu"
      },
      "source": [
        "# train-test split\n",
        "train_text, test_text, train_labels , test_labels = train_test_split(df['text'], df['fraudulent'] , test_size = 0.15)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouNER3B_p9cq",
        "outputId": "304510e0-6800-455d-8cb7-ebd6259f4bd6"
      },
      "source": [
        "# Max number of words in a sequence\n",
        "maxlen = 250\n",
        "\n",
        "# embedding size to be created\n",
        "# This depends on the GLOVE file loaded earlier\n",
        "embed_dim = 50\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)\n",
        "print('Found %s unique tokens.' % vocab_size)\n",
        "vocab_size = vocab_size + 1\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "train_padded = pad_sequences(\n",
        "    train_sequences, maxlen=maxlen, padding=\"post\", truncating=\"post\"\n",
        ")\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
        "test_padded = pad_sequences(\n",
        "    test_sequences, maxlen=maxlen, padding=\"post\", truncating=\"post\"\n",
        ")\n",
        "\n",
        "print(f\"Shape of train {train_padded.shape}\")\n",
        "print(f\"Shape of test {test_padded.shape}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 157210 unique tokens.\n",
            "Shape of train (15198, 250)\n",
            "Shape of test (2682, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsdigN5xaLQL"
      },
      "source": [
        "# Implement multi head self attention as a Keras layer\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Mek1GxaZPP"
      },
      "source": [
        "# Implement a Transformer block as a layer\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h77Q-FsqadIf"
      },
      "source": [
        "# Implement embedding layer\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FDFIIIVasGT"
      },
      "source": [
        "# Create classifier model using transformer layer\n",
        "# embed_dim = 32 # defined above  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model3 = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8W0zyzna5OB",
        "outputId": "9623002c-5237-451c-c58b-e75b2213b8a2"
      },
      "source": [
        "model3.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", metrics.FalsePositives(), metrics.FalseNegatives()])\n",
        "model3.summary()\n",
        "model3.fit(train_padded, train_labels, epochs=5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 250)]             0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 250, 50)           7873050   \n",
            "_________________________________________________________________\n",
            "transformer_block (Transform (None, 250, 50)           13682     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 20)                1020      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 7,887,773\n",
            "Trainable params: 7,887,773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "475/475 [==============================] - 100s 211ms/step - loss: 0.1064 - accuracy: 0.9659 - false_positives: 98.0000 - false_negatives: 420.0000\n",
            "Epoch 2/5\n",
            "475/475 [==============================] - 102s 216ms/step - loss: 0.0222 - accuracy: 0.9926 - false_positives: 44.0000 - false_negatives: 69.0000\n",
            "Epoch 3/5\n",
            "475/475 [==============================] - 101s 212ms/step - loss: 0.0066 - accuracy: 0.9982 - false_positives: 12.0000 - false_negatives: 15.0000\n",
            "Epoch 4/5\n",
            "475/475 [==============================] - 99s 207ms/step - loss: 7.7640e-04 - accuracy: 0.9998 - false_positives: 1.0000 - false_negatives: 2.0000\n",
            "Epoch 5/5\n",
            "475/475 [==============================] - 104s 220ms/step - loss: 1.1689e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fea471a0080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEcsSX4-rko3"
      },
      "source": [
        "pred_soft3 = model3.predict(test_padded)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuNgZv8zXdCD",
        "outputId": "5afb1c25-6c2e-4d65-a7db-b776ecf5d9cb"
      },
      "source": [
        "# pred = np.around(pred_soft, decimals = 0)\n",
        "pred3 = np.where(pred_soft3 > 0.50, 1, 0)\n",
        "\n",
        "acc3 = accuracy_score(pred3, test_labels)\n",
        "f13 = f1_score(pred3, test_labels)\n",
        "\n",
        "cm3 = confusion_matrix(test_labels, pred3)\n",
        "tn3 = cm3[0][0]\n",
        "fn3 = cm3[1][0]\n",
        "tp3 = cm3[1][1]\n",
        "fp3 = cm3[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(acc3), 'F1 score: {:.4f}'.format(f13))\n",
        "print('False Positives: {:.0f}'.format(fp3), 'False Negatives: {:.0f}'.format(fn3))\n",
        "print('Confusion matrix:\\n', cm3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9851 F1 score: 0.8413\n",
            "False Positives: 2 False Negatives: 38\n",
            "Confusion matrix:\n",
            " [[2536    2]\n",
            " [  38  106]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xipPlzz2Azoz",
        "outputId": "5be5fbbd-1ed3-43d8-cfcb-3839a4a4be92"
      },
      "source": [
        "# model 2: the LSTM model\n",
        "model2 = Sequential()\n",
        "\n",
        "# embed_dim = 50\n",
        "hidden_size = 32\n",
        "model2.add(Embedding(vocab_size, embed_dim, input_length=maxlen))\n",
        "model2.add(Bidirectional(LSTM(hidden_size, dropout=0.1, recurrent_dropout=0.1, return_sequences=True)))\n",
        "model2.add(GlobalMaxPool1D())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', metrics.FalsePositives(), metrics.FalseNegatives()])\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 250, 50)           7860550   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 250, 64)           21248     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 7,881,863\n",
            "Trainable params: 7,881,863\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNhgjKQRByGN",
        "outputId": "f48e03b2-4964-48c6-94c6-8d01d8069434"
      },
      "source": [
        "model2.fit(train_padded, train_labels, epochs=5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "475/475 [==============================] - 229s 483ms/step - loss: 0.1027 - accuracy: 0.9682 - false_positives_1: 73.0000 - false_negatives_1: 411.0000\n",
            "Epoch 2/5\n",
            "475/475 [==============================] - 227s 478ms/step - loss: 0.0240 - accuracy: 0.9924 - false_positives_1: 31.0000 - false_negatives_1: 85.0000\n",
            "Epoch 3/5\n",
            "475/475 [==============================] - 231s 487ms/step - loss: 0.0054 - accuracy: 0.9981 - false_positives_1: 11.0000 - false_negatives_1: 18.0000\n",
            "Epoch 4/5\n",
            "475/475 [==============================] - 228s 481ms/step - loss: 7.9433e-04 - accuracy: 0.9997 - false_positives_1: 2.0000 - false_negatives_1: 2.0000\n",
            "Epoch 5/5\n",
            "475/475 [==============================] - 229s 482ms/step - loss: 5.1685e-04 - accuracy: 0.9999 - false_positives_1: 1.0000 - false_negatives_1: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fea43c8e6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X78drq-zCybZ"
      },
      "source": [
        "pred_soft2 = model2.predict(test_padded)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuFK-r5uC09H",
        "outputId": "2e5eba67-0bb4-4059-ef38-2a3e569db335"
      },
      "source": [
        "# pred = np.around(pred_soft, decimals = 0)\n",
        "pred2 = np.where(pred_soft2 > 0.50, 1, 0)\n",
        "\n",
        "acc2 = accuracy_score(pred2, test_labels)\n",
        "f12 = f1_score(pred2, test_labels)\n",
        "\n",
        "cm2 = confusion_matrix(test_labels, pred2)\n",
        "tn2 = cm2[0][0]\n",
        "fn2 = cm2[1][0]\n",
        "tp2 = cm2[1][1]\n",
        "fp2 = cm2[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(acc2), 'F1 score: {:.4f}'.format(f12))\n",
        "print('False Positives: {:.0f}'.format(fp2), 'False Negatives: {:.0f}'.format(fn2))\n",
        "print('Confusion matrix:\\n', cm2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9851 F1 score: 0.8485\n",
            "False Positives: 8 False Negatives: 32\n",
            "Confusion matrix:\n",
            " [[2530    8]\n",
            " [  32  112]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJIwNfj9XR_p",
        "outputId": "588da26f-8112-44bc-d379-4e5261145a74"
      },
      "source": [
        "# model 1: BOW + FCNN model\n",
        "cv = CountVectorizer(strip_accents='unicode', lowercase=True, stop_words='english', dtype=np.int8) \n",
        "cv_train_sparse = cv.fit_transform(train_text)\n",
        "cv_train_dense = sci.sparse.csr_matrix.todense(cv_train_sparse)\n",
        "\n",
        "cv_test_sparse = cv.transform(test_text)\n",
        "cv_test_dense = sci.sparse.csr_matrix.todense(cv_test_sparse)\n",
        "\n",
        "print('BOW for cv_train:', cv_train_dense.shape)\n",
        "print('BOW for cv_test:', cv_test_dense.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW for cv_train: (15198, 150190)\n",
            "BOW for cv_test: (2682, 150190)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGWfDG-KXg5-",
        "outputId": "094be770-607f-448e-e0c0-8b62ccfc4508"
      },
      "source": [
        "\"\"\"\n",
        "Fully connected NN model with two hidden layers \n",
        "\"\"\"\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(units = 100 , activation = 'relu' , input_dim = cv_train_dense.shape[1]))\n",
        "model1.add(Dropout(0.1))\n",
        "model1.add(Dense(units = 10 , activation = 'relu'))\n",
        "model1.add(Dropout(0.1))\n",
        "model1.add(Dense(units = 1 , activation = 'sigmoid'))\n",
        "model1.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy', tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n",
        "model1.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 100)               15019100  \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 15,020,121\n",
            "Trainable params: 15,020,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--t8kVV5XlBy",
        "outputId": "f7b03f3e-d80f-47a0-b15d-5f71ad05af15"
      },
      "source": [
        "model1.fit(cv_train_dense, train_labels, epochs = 5)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "475/475 [==============================] - 50s 106ms/step - loss: 0.0877 - accuracy: 0.9704 - false_positives_2: 81.0000 - false_negatives_2: 369.0000\n",
            "Epoch 2/5\n",
            "475/475 [==============================] - 50s 106ms/step - loss: 0.0173 - accuracy: 0.9945 - false_positives_2: 26.0000 - false_negatives_2: 58.0000\n",
            "Epoch 3/5\n",
            "475/475 [==============================] - 50s 106ms/step - loss: 0.0055 - accuracy: 0.9984 - false_positives_2: 7.0000 - false_negatives_2: 17.0000\n",
            "Epoch 4/5\n",
            "475/475 [==============================] - 50s 106ms/step - loss: 0.0028 - accuracy: 0.9993 - false_positives_2: 5.0000 - false_negatives_2: 6.0000\n",
            "Epoch 5/5\n",
            "475/475 [==============================] - 50s 105ms/step - loss: 0.0030 - accuracy: 0.9989 - false_positives_2: 11.0000 - false_negatives_2: 6.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fea41ba5710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67hb8V7sXlfK"
      },
      "source": [
        "pred_soft1 = model1.predict(cv_test_dense)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLIwh073XpxW",
        "outputId": "b9438683-551a-442f-c5f6-546fb89207a7"
      },
      "source": [
        "# pred = np.around(pred_soft, decimals = 0)\n",
        "pred1 = np.where(pred_soft1 > 0.50, 1, 0)\n",
        "\n",
        "acc1 = accuracy_score(pred1, test_labels)\n",
        "f11 = f1_score(pred1, test_labels)\n",
        "\n",
        "cm1 = confusion_matrix(test_labels, pred1)\n",
        "tn1 = cm1[0][0]\n",
        "fn1 = cm1[1][0]\n",
        "tp1 = cm1[1][1]\n",
        "fp1 = cm1[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(acc1), 'F1 score: {:.4f}'.format(f11))\n",
        "print('False Positives: {:.0f}'.format(fp1), 'False Negatives: {:.0f}'.format(fn1))\n",
        "print('Confusion matrix:\\n', cm1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9862 F1 score: 0.8538\n",
            "False Positives: 1 False Negatives: 36\n",
            "Confusion matrix:\n",
            " [[2537    1]\n",
            " [  36  108]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBvLWsMbYgLj",
        "outputId": "0831a017-ffab-44cd-983b-25fa103673ae"
      },
      "source": [
        "# Averaging ensemble prediction\n",
        "pred_softa=(pred_soft1 + pred_soft2 + pred_soft3)/3\n",
        "\n",
        "# Set probability to declare post as fraudulent\n",
        "preda = np.where(pred_softa > 0.15, 1, 0)\n",
        "\n",
        "acca = accuracy_score(preda, test_labels)\n",
        "f1a = f1_score(preda, test_labels)\n",
        "\n",
        "cma = confusion_matrix(test_labels, preda)\n",
        "tna = cmf[0][0]\n",
        "fna = cmf[1][0]\n",
        "tpa = cmf[1][1]\n",
        "fpa = cmf[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(acca), 'F1 score: {:.4f}'.format(f1a))\n",
        "print('False Positives: {:.0f}'.format(fpa), 'False Negatives: {:.0f}'.format(fna))\n",
        "print('Confusion matrix:\\n', cma)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9881 F1 score: 0.8841\n",
            "False Positives: 10 False Negatives: 22\n",
            "Confusion matrix:\n",
            " [[2528   10]\n",
            " [  22  122]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCEoMit7nNGe"
      },
      "source": [
        "# stacking ensemble\n",
        "\n",
        "# get prediction scores for train samples\n",
        "train_soft1 = model1.predict(cv_train_dense)\n",
        "train_soft2 = model2.predict(train_padded)\n",
        "train_soft3 = model3.predict(train_padded)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3UDyxO-ua_Q"
      },
      "source": [
        "stack_train = np.hstack([train_soft1, train_soft2, train_soft3])\n",
        "\n",
        "stack_test = np.hstack([pred_soft1, pred_soft2, pred_soft3])\n",
        "\n",
        "model_stack = LogisticRegression()\n",
        "model_stack.fit(stack_train, train_labels)\n",
        "\n",
        "pred_softs = model_stack.predict(stack_test)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRXDpGvVsM6q",
        "outputId": "383090fd-af54-4b72-9e1c-c53eec60e126"
      },
      "source": [
        "# Stacking prediction\n",
        "\n",
        "# Set probability to declare post as fraudulent\n",
        "preds = np.where(pred_softs > 0.01, 1, 0)\n",
        "\n",
        "accs = accuracy_score(preds, test_labels)\n",
        "f1s = f1_score(preds, test_labels)\n",
        "\n",
        "cms = confusion_matrix(test_labels, preds)\n",
        "tns = cms[0][0]\n",
        "fns = cms[1][0]\n",
        "tps = cms[1][1]\n",
        "fps = cms[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(accs), 'F1 score: {:.4f}'.format(f1s))\n",
        "print('False Positives: {:.0f}'.format(fps), 'False Negatives: {:.0f}'.format(fns))\n",
        "print('Confusion matrix:\\n', cms)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9855 F1 score: 0.8446\n",
            "False Positives: 1 False Negatives: 38\n",
            "Confusion matrix:\n",
            " [[2537    1]\n",
            " [  38  106]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}